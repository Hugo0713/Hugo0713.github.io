<!doctypehtml><html lang=zh-CN><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 7.3.0" name=generator><link href=/images/apple-touch-icon-next.png rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><link href=/css/main.css rel=stylesheet><link href=/lib/font-awesome/css/all.min.css rel=stylesheet><link href=/lib/pace/pace-theme-material.min.css rel=stylesheet><script src=/lib/pace/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":280,"display":"post","padding":18,"offset":15,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#29c6c0","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":false,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><meta content="本文在deepseek辅助下帮助笔者理解移动端人工智能的知识蒸馏（Knowledge Distillation）、量化（Quantization）和剪枝（Pruning）三种模型压缩技术。" name=description><meta content=article property=og:type><meta content=移动端人工智能技术 property=og:title><meta content=http://example.com/2025/02/23/Technology/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF/index.html property=og:url><meta content="Hugo's Cyber Heritage" property=og:site_name><meta content="本文在deepseek辅助下帮助笔者理解移动端人工智能的知识蒸馏（Knowledge Distillation）、量化（Quantization）和剪枝（Pruning）三种模型压缩技术。" property=og:description><meta content=zh_CN property=og:locale><meta content=2025-02-23T12:40:58.000Z property=article:published_time><meta content=2025-02-27T11:26:35.685Z property=article:modified_time><meta content=Hugo property=article:author><meta content=AI辅助 property=article:tag><meta content=端侧AI property=article:tag><meta content=summary name=twitter:card><link href=http://example.com/2025/02/23/Technology/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>移动端人工智能技术 | Hugo's Cyber Heritage</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><!-- hexo injector head_end start --><style>.s {
    background-color: #000;
    color: #000;
    padding: 0 3px;
    cursor: pointer;
    user-select: none;
  }
  .s.revealed {
    background-color: transparent;
    color: inherit;
  }</style><!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}</style><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style><body itemscope itemtype=http://schema.org/WebPage><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Hugo's Cyber Heritage</h1> <span class=logo-line-after><i></i></span> </a><p class=site-subtitle itemprop=description>Shine or Die</div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类<span class=badge>12</span></a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签<span class=badge>32</span></a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档<span class=badge>58</span></a><li class="menu-item menu-item-resources"><a href=/resources/ rel=section><i class="fa fa-book fa-fw"></i>资源</a><li class="menu-item menu-item-contact"><a href=/contact/ rel=section><i class="fa fa-envelope fa-fw"></i>联系</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input placeholder=搜索... spellcheck=false type=search></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div id=search-result><div id=no-result><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class=reading-progress-bar></div><a class="book-mark-link book-mark-link-fixed" role=button></a><a aria-label="Follow me on GitHub" title="Follow me on GitHub" class=github-corner href=https://github.com/hugo0713 rel=noopener target=_blank><svg viewbox="0 0 250 250" aria-hidden=true height=80 width=80><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" style="transform-origin: 130px 106px;" class=octo-arm fill=currentColor></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" class=octo-body fill=currentColor></path></svg></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=http://example.com/2025/02/23/Technology/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/images/avatar.png itemprop=image> <meta content=Hugo itemprop=name> <meta content=古之成大事者，不惟有超世之才，亦必有坚韧不拔之志。 itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="Hugo's Cyber Heritage" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>移动端人工智能技术</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2025-02-23 20:40:58" datetime=2025-02-23T20:40:58+08:00>2025-02-23</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2025-02-27 19:26:35" datetime=2025-02-27T19:26:35+08:00 itemprop=dateModified>2025-02-27</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>分类于</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/Technology/ itemprop=url rel=index><span itemprop=name>Technology</span></a> </span> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>7.2k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>7 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>本文在deepseek辅助下帮助笔者理解移动端人工智能的知识蒸馏（Knowledge Distillation）、量化（Quantization）和剪枝（Pruning）三种模型压缩技术。<br><span id=more></span><h3 id=1-知识蒸馏（Knowledge-Dististillation）><a title="1. 知识蒸馏（Knowledge Dististillation）" class=headerlink href=#1-知识蒸馏（Knowledge-Dististillation）></a><strong>1. 知识蒸馏（Knowledge Dististillation）</strong></h3><h4 id=核心原理><a class=headerlink href=#核心原理 title=核心原理></a><strong>核心原理</strong></h4><p>通过训练一个轻量化的“学生模型”（Student Model），模仿复杂“教师模型”（Teacher Model）的输出行为，从而将教师模型的知识迁移到学生模型中。<ul><li><strong>知识来源</strong>：教师模型的输出概率分布（软标签）、中间层特征或注意力机制。<li><strong>目标</strong>：学生模型在保持小体积的同时，达到接近教师模型的性能。</ul><h4 id=示例><a class=headerlink href=#示例 title=示例></a><strong>示例</strong></h4><p><strong>场景</strong>：图像分类任务（如ImageNet数据集）<ul><li><strong>教师模型</strong>：大型模型（如ResNet-50，准确率76%）。<li><strong>学生模型</strong>：轻量模型（如MobileNetV3，准确率直接训练仅70%）。<li><strong>蒸馏过程</strong>： <ol><li>教师模型对训练数据生成“软标签”（Soft Labels，即各类别概率分布，如<code>[0.7, 0.2, 0.1]</code>）。<li>学生模型同时学习真实标签（硬标签）和软标签。<br>结合硬标签损失和软标签的KL散度损失函数：<script type="math/tex; mode=display">
L = \alpha L_{CE} + (1 - \alpha) L_{KL}</script>其中，<script type=math/tex>L_{CE}</script>为交叉熵损失，<script type=math/tex>L_{KL}</script>为KL散度损失。</ol></ul><h4 id=深入思考><a class=headerlink href=#深入思考 title=深入思考></a>深入思考</h4><p>1.<strong>为什么学生模型参数更少却能接近教师性能？</strong><p>类比于二级结论，学生模型具有教师模型的先验知识（概率分布），而不需要从底层开始全部学习。我们称之为<strong>决策边界抽象能力</strong>。<p>信息论角度：将教师模型中“有效信息”（决策边界、特征相关性）编码到学生模型的参数中，而非复制所有参数。<p>2.<strong>软标签概率分布如何生成？</strong><p>核心方法：温度缩放（Temperature Scaling）<br>软标签并非直接使用教师模型的原始输出，而是通过引入温度参数（Temperature, T）对概率分布进行平滑处理，以传递类别间的关系信息。<p>数学公式：</p><script type="math/tex; mode=display">
p_i = \frac{e^{\frac{z_i}{T}}}{\sum_{j=1}^{C} e^{\frac{z_j}{T}}}</script><p>其中，<mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.357ex;" viewbox="0 -442 792 599.8" focusable=false height=1.357ex role=img width=1.792ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=msub><g data-mml-node=mi><path d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" data-c=1D467></path></g><g transform="translate(498,-150) scale(0.707)" data-mml-node=mi><path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" data-c=1D456></path></g></g></g></g></svg></mjx-container>是教师模型在类别<mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.025ex;" viewbox="0 -661 345 672" focusable=false height=1.52ex role=img width=0.781ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" data-c=1D456></path></g></g></g></svg></mjx-container>的原始输出，<mjx-container class=MathJax jax=SVG><svg style="vertical-align: 0;" viewbox="0 -677 704 677" focusable=false height=1.532ex role=img width=1.593ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" data-c=1D447></path></g></g></g></svg></mjx-container>是温度参数，<mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.05ex;" viewbox="0 -705 760 727" focusable=false height=1.645ex role=img width=1.719ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" data-c=1D436></path></g></g></g></svg></mjx-container>是类别总数。<p>T的作用：<ul><li>当 <mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.186ex;" viewbox="0 -677 2537.6 759" focusable=false height=1.717ex role=img width=5.741ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" data-c=1D447></path></g><g data-mml-node=mo transform=translate(981.8,0)><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c=3D></path></g><g data-mml-node=mn transform=translate(2037.6,0)><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c=31></path></g></g></g></svg></mjx-container> 时，软标签等同于硬标签。<li>当 <mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.09ex;" viewbox="0 -677 2537.6 717" focusable=false height=1.622ex role=img width=5.741ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" data-c=1D447></path></g><g data-mml-node=mo transform=translate(981.8,0)><path d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z" data-c=3E></path></g><g data-mml-node=mn transform=translate(2037.6,0)><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c=31></path></g></g></g></svg></mjx-container> 时，概率分布更平滑，类别间关系信息更丰富。<li>当 <mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.025ex;" viewbox="0 -677 3259.6 688" focusable=false height=1.557ex role=img width=7.375ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" data-c=1D447></path></g><g data-mml-node=mo transform=translate(981.8,0)><path d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z" data-c=2192></path></g><g data-mml-node=mi transform=translate(2259.6,0)><path d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z" data-c=221E></path></g></g></g></svg></mjx-container> 时，概率分布趋近于均匀分布。</ul><p>3.<strong>为什么不在训练教师模型时使用软标签？</strong><p>根本原因：教师模型的训练目标不同<ul><li>教师模型的使命：追求最高精度，而非传递知识<ul><li>教师模型需尽可能拟合数据中的细节，硬标签（明确答案）是更直接的监督信号。<li>软标签会引入不必要的“不确定性”，降低模型对正确类别的置信度。</ul><li>软标签的来源矛盾：<ul><li>知识蒸馏中，软标签由更强大的教师模型生成（例如ResNet-50教MobileNet）。<li>若在训练教师模型时使用软标签，需要另一个更强的模型生成软标签，但这会导致无限递归问题（谁来生成这个更强的模型的软标签？）</ul></ul><p>4.<strong>知识蒸馏的局限性</strong><ul><li>需要高质量的教师模型：教师模型需要足够大，才能提供高质量的软标签。<li>需要大量计算资源：教师模型需要大量计算资源，才能生成高质量的软标签。<li>需要大量数据：教师模型需要大量数据，才能生成高质量的软标签。</ul><hr><h3 id=2-量化（Quantization）><a title="2. 量化（Quantization）" class=headerlink href=#2-量化（Quantization）></a><strong>2. 量化（Quantization）</strong></h3><h4 id=核心原理-1><a class=headerlink href=#核心原理-1 title=核心原理></a><strong>核心原理</strong></h4><p>将模型参数（权重）和激活值从高精度浮点数（如32位）转换为低精度数值（如8位整数），减少模型体积和计算资源消耗。<ul><li><strong>类型</strong>： <ul><li><strong>训练后量化（Post-training Quantization）</strong>：直接对训练好的模型进行量化。<li><strong>量化感知训练（Quantization-aware Training）</strong>：在训练过程中模拟量化误差，提升最终量化模型的精度。</ul></ul><h4 id=示例-1><a class=headerlink href=#示例-1 title=示例></a><strong>示例</strong></h4><p><strong>场景</strong>：手机端语音识别模型<ul><li><strong>原始模型</strong>：基于LSTM的语音识别模型，使用FP32精度，大小120MB，延迟50ms。<li><strong>量化步骤</strong>： <ol><li>将权重和激活值从FP32量化为INT8（范围映射到-128~127）。<li>引入反量化（Dequantization）层，在关键计算节点恢复精度。</ol><li><strong>结果</strong>：模型大小缩减至30MB，延迟降至15ms，准确率损失小于1%。</ul><p>下面以Pytorch为例，展示训练后量化和量化感知训练的实现。<h5 id=训练后量化><a class=headerlink href=#训练后量化 title=训练后量化></a>训练后量化</h5><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.quantization</span><br><span class=line><span class=keyword>from</span> torchvision.models <span class=keyword>import</span> mobilenet_v2</span><br><span class=line></span><br><span class=line><span class=comment># Step 1: 加载预训练模型</span></span><br><span class=line>model = mobilenet_v2(pretrained=<span class=literal>True</span>)</span><br><span class=line>model.<span class=built_in>eval</span>()</span><br><span class=line></span><br><span class=line><span class=comment># Step 2: 定义量化配置</span></span><br><span class=line>model.qconfig = torch.quantization.get_default_qconfig(<span class=string>'qnnpack'</span>)  <span class=comment># 移动端优化配置</span></span><br><span class=line></span><br><span class=line><span class=comment># Step 3: 插入观察器（Observer）校准量化参数</span></span><br><span class=line>model_fp32_prepared = torch.quantization.prepare(model)</span><br><span class=line></span><br><span class=line><span class=comment># Step 4: 用校准数据运行模型（此处用随机数据示例）</span></span><br><span class=line>input_fp32 = torch.randn(<span class=number>1</span>, <span class=number>3</span>, <span class=number>224</span>, <span class=number>224</span>)  <span class=comment># 假设输入尺寸为224x224</span></span><br><span class=line><span class=keyword>with</span> torch.no_grad():</span><br><span class=line>    model_fp32_prepared(input_fp32)</span><br><span class=line></span><br><span class=line><span class=comment># Step 5: 转换为量化模型</span></span><br><span class=line>model_int8 = torch.quantization.convert(model_fp32_prepared)</span><br><span class=line></span><br><span class=line><span class=comment># 保存量化模型</span></span><br><span class=line>torch.save(model_int8.state_dict(), <span class=string>"mobilenet_v2_quantized.pth"</span>)</span><br><span class=line></span><br><span class=line><span class=comment># 检查模型大小</span></span><br><span class=line><span class=keyword>import</span> os</span><br><span class=line><span class=built_in>print</span>(<span class=string>"FP32模型大小:"</span>, os.path.getsize(<span class=string>"mobilenet_v2.pth"</span>)/<span class=number>1e6</span>, <span class=string>"MB"</span>)     <span class=comment># 约14MB</span></span><br><span class=line><span class=built_in>print</span>(<span class=string>"INT8模型大小:"</span>, os.path.getsize(<span class=string>"mobilenet_v2_quantized.pth"</span>)/<span class=number>1e6</span>, <span class=string>"MB"</span>)  <span class=comment># 约3.5MB</span></span><br></pre></table></figure><h5 id=量化感知训练><a class=headerlink href=#量化感知训练 title=量化感知训练></a>量化感知训练</h5><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>from</span> torch.quantization <span class=keyword>import</span> QuantStub, DeQuantStub</span><br><span class=line></span><br><span class=line><span class=comment># Step 1: 定义支持量化的模型结构</span></span><br><span class=line><span class=keyword>class</span> <span class="title class_">QuantizableModel</span>(nn.Module):</span><br><span class=line>    <span class=keyword>def</span> <span class="title function_">__init__</span>(<span class=params>self</span>):</span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        <span class="variable language_">self</span>.quant = QuantStub()      <span class=comment># 量化入口</span></span><br><span class=line>        <span class="variable language_">self</span>.conv = nn.Conv2d(<span class=number>3</span>, <span class=number>64</span>, kernel_size=<span class=number>3</span>)</span><br><span class=line>        <span class="variable language_">self</span>.dequant = DeQuantStub()  <span class=comment># 反量化出口</span></span><br><span class=line>    </span><br><span class=line>    <span class=keyword>def</span> <span class="title function_">forward</span>(<span class=params>self, x</span>):</span><br><span class=line>        x = <span class="variable language_">self</span>.quant(x)</span><br><span class=line>        x = <span class="variable language_">self</span>.conv(x)</span><br><span class=line>        x = <span class="variable language_">self</span>.dequant(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line><span class=comment># Step 2: 插入伪量化节点</span></span><br><span class=line>model = QuantizableModel()</span><br><span class=line>model.qconfig = torch.quantization.get_default_qat_qconfig(<span class=string>'qnnpack'</span>)</span><br><span class=line>model.train()  <span class=comment># 切换到训练模式</span></span><br><span class=line>model_prepared = torch.quantization.prepare_qat(model)</span><br><span class=line></span><br><span class=line><span class=comment># Step 3: 正常训练流程（需使用FP32数据）</span></span><br><span class=line>optimizer = torch.optim.SGD(model_prepared.parameters(), lr=<span class=number>0.001</span>)</span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>10</span>):</span><br><span class=line>    <span class=keyword>for</span> data, target <span class=keyword>in</span> train_loader:  <span class=comment># 假设已有数据加载器</span></span><br><span class=line>        optimizer.zero_grad()</span><br><span class=line>        output = model_prepared(data)</span><br><span class=line>        loss = nn.CrossEntropyLoss()(output, target)</span><br><span class=line>        loss.backward()</span><br><span class=line>        optimizer.step()</span><br><span class=line></span><br><span class=line><span class=comment># Step 4: 转换为最终量化模型</span></span><br><span class=line>model_int8 = torch.quantization.convert(model_prepared)</span><br></pre></table></figure><h4 id=深入思考-1><a class=headerlink href=#深入思考-1 title=深入思考></a>深入思考</h4><p>手动实现量化计算：<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br></pre><td class=code><pre><span class=line><span class=comment># 原始FP32计算</span></span><br><span class=line>W_fp32 = torch.tensor([<span class=number>2.5</span>, -<span class=number>1.3</span>, <span class=number>0.8</span>], dtype=torch.float32)</span><br><span class=line>x_fp32 = torch.tensor([<span class=number>0.4</span>, <span class=number>1.2</span>, -<span class=number>0.5</span>], dtype=torch.float32)</span><br><span class=line>y_fp32 = torch.dot(W_fp32, x_fp32)  <span class=comment># 输出：2.5*0.4 + (-1.3)*1.2 + 0.8*(-0.5) = -1.56</span></span><br><span class=line></span><br><span class=line><span class=comment># 量化到INT8（范围假设为[-5, 5]）</span></span><br><span class=line>scale_W = <span class=number>5</span> / <span class=number>127</span>  <span class=comment># 对称量化，scale = max(abs(W)) / 127</span></span><br><span class=line>W_int8 = torch.clamp((W_fp32 / scale_W).<span class=built_in>round</span>(), <span class=built_in>min</span>=-<span class=number>128</span>, <span class=built_in>max</span>=<span class=number>127</span>).to(torch.int8)</span><br><span class=line>scale_x = <span class=number>5</span> / <span class=number>127</span></span><br><span class=line>x_int8 = torch.clamp((x_fp32 / scale_x).<span class=built_in>round</span>(), <span class=built_in>min</span>=-<span class=number>128</span>, <span class=built_in>max</span>=<span class=number>127</span>).to(torch.int8)</span><br><span class=line></span><br><span class=line><span class=comment># 整数计算</span></span><br><span class=line>y_int32 = torch.dot(W_int8.<span class=built_in>float</span>(), x_int8.<span class=built_in>float</span>())  <span class=comment># 转为float避免溢出</span></span><br><span class=line>y_dequant = y_int32 * (scale_W * scale_x)  <span class=comment># 反量化</span></span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(<span class=string>"FP32结果:"</span>, y_fp32.item())        <span class=comment># -1.56</span></span><br><span class=line><span class=built_in>print</span>(<span class=string>"量化结果:"</span>, y_dequant.item())     <span class=comment># 约-1.55（存在微小误差）</span></span><br></pre></table></figure><p>量化完整流程：<ul><li>准备阶段<ul><li>插入观察器到模型中，统计各层的权重和激活值分布。<li>代码操作：model_prepared = prepare(model)</ul><li>校准阶段<ul><li>用代表性数据运行模型，观察器记录各层的min/max值。<li>代码操作：model_prepared(input_data)</ul><li>转换阶段<ul><li>根据校准结果计算量化参数，替换浮点算子为量化算子。<li>代码操作：model_quantized = convert(model_prepared)</ul></ul><p><strong>核心公式</strong>：<ul><li>量化公式：<script type=math/tex>Q(x) = \text{clamp}\left(\text{round}\left(\frac{x}{\text{scale}} + \text{zero\_point}\right), \text{min}, \text{max}\right)</script><ul><li>clamp：将结果限制在min和max之间<li>round：四舍五入<li>zero_point：量化偏移量, 用于校准, 通常为0</ul><li>反量化公式：<script type=math/tex>x = \left(\text{Q}(x) - \text{zero\_point}\right) \times \text{scale}</script></ul><h4 id=实际应用><a class=headerlink href=#实际应用 title=实际应用></a><strong>实际应用</strong></h4><ul><li>TensorFlow Lite：默认支持训练后量化，可将目标检测模型（如SSD MobileNet）从16MB压缩到4MB。<li>苹果Core ML：在iPhone上运行量化后的StyleGAN模型，实现实时人像风格迁移。</ul><hr><h3 id=3-剪枝（Pruning）><a title="3. 剪枝（Pruning）" class=headerlink href=#3-剪枝（Pruning）></a><strong>3. 剪枝（Pruning）</strong></h3><h4 id=核心原理-2><a class=headerlink href=#核心原理-2 title=核心原理></a><strong>核心原理</strong></h4><p>通过移除模型中不重要的参数（如接近零的权重）或结构（如冗余神经元），减少模型复杂度。<ul><li><strong>类型</strong>： <ul><li><strong>非结构化剪枝</strong>：删除单个权重（稀疏化）。<li><strong>结构化剪枝</strong>：删除整层神经元或通道（更适合硬件加速）。</ul></ul><h4 id=示例-2><a class=headerlink href=#示例-2 title=示例></a><strong>示例</strong></h4><p><strong>场景</strong>：自然语言处理中的BERT模型压缩<ul><li><strong>原始模型</strong>：BERT-base（1.1亿参数，模型大小400MB）。<li><strong>剪枝过程</strong>： <ol><li>在微调阶段，根据权重绝对值或梯度重要性评分，剪枝30%的注意力头。<li>重新训练剩余参数以恢复精度。</ol><li><strong>结果</strong>：模型大小减少至280MB，推理速度提升1.5倍，在GLUE基准上精度下降仅0.5%。</ul><p>以下是Pytorch实现剪枝的示例：<h5 id=非结构化剪枝><a class=headerlink href=#非结构化剪枝 title=非结构化剪枝></a>非结构化剪枝</h5><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.utils.prune <span class=keyword>as</span> prune</span><br><span class=line></span><br><span class=line><span class=comment># 定义示例模型</span></span><br><span class=line><span class=keyword>class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class=line>    <span class=keyword>def</span> <span class="title function_">__init__</span>(<span class=params>self</span>):</span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class=number>3</span>, <span class=number>16</span>, <span class=number>3</span>)  <span class=comment># 输入通道3，输出通道16</span></span><br><span class=line>        <span class="variable language_">self</span>.fc = nn.Linear(<span class=number>16</span>*<span class=number>26</span>*<span class=number>26</span>, <span class=number>10</span>)  <span class=comment># 假设输入图像尺寸为28x28</span></span><br><span class=line></span><br><span class=line>    <span class=keyword>def</span> <span class="title function_">forward</span>(<span class=params>self, x</span>):</span><br><span class=line>        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class=line>        x = x.view(x.size(<span class=number>0</span>), -<span class=number>1</span>)</span><br><span class=line>        x = <span class="variable language_">self</span>.fc(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line>model = SimpleCNN()</span><br><span class=line></span><br><span class=line><span class=comment># --- 剪枝步骤 ---</span></span><br><span class=line><span class=comment># Step 1: 选择剪枝目标（这里剪枝conv1层的权重）</span></span><br><span class=line>parameters_to_prune = [(model.conv1, <span class=string>'weight'</span>)]</span><br><span class=line></span><br><span class=line><span class=comment># Step 2: 应用L1范数剪枝（剪去20%的权重）</span></span><br><span class=line>prune.global_unstructured(</span><br><span class=line>    parameters_to_prune,</span><br><span class=line>    pruning_method=prune.L1Unstructured,</span><br><span class=line>    amount=<span class=number>0.2</span>  <span class=comment># 剪枝比例20%</span></span><br><span class=line>)</span><br><span class=line></span><br><span class=line><span class=comment># Step 3: 查看剪枝效果</span></span><br><span class=line><span class=built_in>print</span>(<span class=string>"剪枝后的权重稀疏度："</span>, </span><br><span class=line>      torch.<span class=built_in>sum</span>(model.conv1.weight == <span class=number>0</span>).item() / model.conv1.weight.nelement())</span><br><span class=line></span><br><span class=line><span class=comment># Step 4: 永久移除剪枝的权重（可选）</span></span><br><span class=line>prune.remove(model.conv1, <span class=string>'weight'</span>)</span><br><span class=line></span><br><span class=line><span class=comment># Step 5: 微调剪枝后的模型</span></span><br><span class=line>optimizer = torch.optim.Adam(model.parameters(), lr=<span class=number>1e-3</span>)</span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>5</span>):</span><br><span class=line>    <span class=keyword>for</span> data, target <span class=keyword>in</span> train_loader:  <span class=comment># 假设已有数据加载器</span></span><br><span class=line>        optimizer.zero_grad()</span><br><span class=line>        output = model(data)</span><br><span class=line>        loss = nn.CrossEntropyLoss()(output, target)</span><br><span class=line>        loss.backward()</span><br><span class=line>        optimizer.step()</span><br><span class=line></span><br></pre></table></figure><p><strong>为什么用L1范数剪枝？</strong>：<br>L1范数具有自然的稀疏性特征，通过最小化L1范数，模型倾向于将一些权重推向0以实现稀疏化，并且计算简单。<h5 id=结构化剪枝><a class=headerlink href=#结构化剪枝 title=结构化剪枝></a>结构化剪枝</h5><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> torch.nn.utils.prune <span class=keyword>import</span> ln_structured, remove_structured</span><br><span class=line></span><br><span class=line><span class=comment># Step 1: 剪枝整个通道（基于L2范数）</span></span><br><span class=line><span class=comment># 对conv1层的输出通道进行剪枝（移除20%的通道）</span></span><br><span class=line>prune.ln_structured(</span><br><span class=line>    model.conv1,</span><br><span class=line>    name=<span class=string>"weight"</span>,</span><br><span class=line>    amount=<span class=number>0.2</span>,</span><br><span class=line>    n=<span class=number>2</span>,  <span class=comment># L2范数</span></span><br><span class=line>    dim=<span class=number>0</span>  <span class=comment># 沿输出通道维度剪枝</span></span><br><span class=line>)</span><br><span class=line></span><br><span class=line><span class=comment># Step 2: 查看通道剪枝后的权重形状</span></span><br><span class=line><span class=built_in>print</span>(<span class=string>"剪枝后的conv1.weight形状:"</span>, model.conv1.weight.shape)  </span><br><span class=line><span class=comment># 原始形状[16,3,3,3] → 剪枝后[13,3,3,3]（假设移除3个通道）</span></span><br><span class=line></span><br><span class=line><span class=comment># Step 3: 永久应用剪枝</span></span><br><span class=line>remove_structured(model.conv1, <span class=string>'weight'</span>)</span><br><span class=line></span><br><span class=line><span class=comment># Step 4: 调整后续层（重要！结构化剪枝需适配网络结构）</span></span><br><span class=line><span class=comment># 原fc层输入维度为16*26*26，剪枝后变为13*26*26 → 需要重新定义</span></span><br><span class=line>model.fc = nn.Linear(<span class=number>13</span>*<span class=number>26</span>*<span class=number>26</span>, <span class=number>10</span>)  <span class=comment># 修改输入维度</span></span><br><span class=line></span><br><span class=line><span class=comment># 微调模型（同上）</span></span><br></pre></table></figure><p><strong>为什么用L2范数剪枝？</strong>：<ul><li>避免极端值：均匀缩小<li>计算效率：L2范数计算复杂度较低</ul><h4 id=实际应用-1><a class=headerlink href=#实际应用-1 title=实际应用></a><strong>实际应用</strong></h4><ul><li>NVIDIA的Nemo框架：对语音识别模型（如QuartzNet）进行结构化剪枝，GPU推理速度提升2倍。<li>无人机避障算法：剪枝后的YOLOv5模型在边缘设备上实时检测障碍物，功耗降低40%。</ul><hr><h3 id=三者的对比与协同使用><a class=headerlink href=#三者的对比与协同使用 title=三者的对比与协同使用></a><strong>三者的对比与协同使用</strong></h3><div class=table-container><table><thead><tr><th>技术<th>核心目标<th>优势<th>局限性<th>典型压缩率<tbody><tr><td>知识蒸馏<td>迁移知识到小模型<td>精度接近教师模型<td>依赖高质量教师模型<td>2-5倍<tr><td>量化<td>降低数值精度<td>显著减少体积和计算开销<td>可能损失精度（需校准）<td>4倍+<tr><td>剪枝<td>移除冗余参数或结构<td>提升推理速度，降低内存占用<td>可能破坏模型结构完整性<td>2-10倍</table></div><p><strong>协同使用案例</strong>：<br>谷歌的MobileNetV4模型结合三者：<ol><li>用知识蒸馏从EfficientNet迁移知识；<li>对模型进行混合精度量化（部分层用INT8，关键层用FP16）；<li>剪枝掉80%的冗余通道，最终模型体积减少6倍，速度提升3倍，精度仅下降2%。</ol><hr><h3 id=总结><a class=headerlink href=#总结 title=总结></a><strong>总结</strong></h3><p>知识蒸馏、量化和剪枝是移动端AI模型压缩的三大核心技术：<ul><li><strong>知识蒸馏</strong>：通过“师生学习”传递知识，适合模型功能迁移；<li><strong>量化</strong>：降低数值精度，直接压缩体积和加速计算；<li><strong>剪枝</strong>：消除冗余参数，提升硬件执行效率。<br>实际应用中，三者常结合使用（如“蒸馏+量化+剪枝”流程），在保证精度的前提下，实现移动端AI模型的极致优化。</ul></div><div class=reward-container><div></div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Hugo 微信支付" src=/images/wechatpay.png><p>微信支付</div><div style="display: inline-block;"><img alt="Hugo 支付宝" src=/images/alipay.png><p>支付宝</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Hugo<li class=post-copyright-link><strong>本文链接：</strong> <a href=http://example.com/2025/02/23/Technology/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF/ title=移动端人工智能技术>http://example.com/2025/02/23/Technology/移动端人工智能技术/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><footer class=post-footer><div class=post-tags><a href=/tags/AI%E8%BE%85%E5%8A%A9/ rel=tag># AI辅助</a><a href=/tags/%E7%AB%AF%E4%BE%A7AI/ rel=tag># 端侧AI</a></div><div class=post-nav><div class=post-nav-item><a title="NVIDIA DPU Course" href=/2025/02/22/Technology/NVIDIA-DPU-Course/ rel=prev> <i class="fa fa-chevron-left"></i> NVIDIA DPU Course </a></div><div class=post-nav-item><a href=/2025/02/24/Networking/%E8%AE%A1%E7%BD%91-%E7%89%A9%E7%90%86%E5%B1%82/ rel=next title=计网-物理层> 计网-物理层 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><div class=comments id=gitalk-container></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-3"><a class=nav-link href=#1-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%EF%BC%88Knowledge-Dististillation%EF%BC%89><span class=nav-number>1.</span> <span class=nav-text>1. 知识蒸馏（Knowledge Dististillation）</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86><span class=nav-number>1.1.</span> <span class=nav-text>核心原理</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%A4%BA%E4%BE%8B><span class=nav-number>1.2.</span> <span class=nav-text>示例</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B7%B1%E5%85%A5%E6%80%9D%E8%80%83><span class=nav-number>1.3.</span> <span class=nav-text>深入思考</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#2-%E9%87%8F%E5%8C%96%EF%BC%88Quantization%EF%BC%89><span class=nav-number>2.</span> <span class=nav-text>2. 量化（Quantization）</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86-1><span class=nav-number>2.1.</span> <span class=nav-text>核心原理</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%A4%BA%E4%BE%8B-1><span class=nav-number>2.2.</span> <span class=nav-text>示例</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E8%AE%AD%E7%BB%83%E5%90%8E%E9%87%8F%E5%8C%96><span class=nav-number>2.2.1.</span> <span class=nav-text>训练后量化</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%E8%AE%AD%E7%BB%83><span class=nav-number>2.2.2.</span> <span class=nav-text>量化感知训练</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B7%B1%E5%85%A5%E6%80%9D%E8%80%83-1><span class=nav-number>2.3.</span> <span class=nav-text>深入思考</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8><span class=nav-number>2.4.</span> <span class=nav-text>实际应用</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#3-%E5%89%AA%E6%9E%9D%EF%BC%88Pruning%EF%BC%89><span class=nav-number>3.</span> <span class=nav-text>3. 剪枝（Pruning）</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86-2><span class=nav-number>3.1.</span> <span class=nav-text>核心原理</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%A4%BA%E4%BE%8B-2><span class=nav-number>3.2.</span> <span class=nav-text>示例</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E5%89%AA%E6%9E%9D><span class=nav-number>3.2.1.</span> <span class=nav-text>非结构化剪枝</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E7%BB%93%E6%9E%84%E5%8C%96%E5%89%AA%E6%9E%9D><span class=nav-number>3.2.2.</span> <span class=nav-text>结构化剪枝</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8-1><span class=nav-number>3.3.</span> <span class=nav-text>实际应用</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%B8%89%E8%80%85%E7%9A%84%E5%AF%B9%E6%AF%94%E4%B8%8E%E5%8D%8F%E5%90%8C%E4%BD%BF%E7%94%A8><span class=nav-number>4.</span> <span class=nav-text>三者的对比与协同使用</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%80%BB%E7%BB%93><span class=nav-number>5.</span> <span class=nav-text>总结</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Hugo class=site-author-image itemprop=image src=/images/avatar.png><p class=site-author-name itemprop=name>Hugo<div class=site-description itemprop=description>古之成大事者，不惟有超世之才，亦必有坚韧不拔之志。</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>58</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>12</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>32</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="GitHub → https://github.com/Hugo0713" href=https://github.com/Hugo0713 rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:hugo0713@sjtu.edu.cn" href=mailto:hugo0713@sjtu.edu.cn rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="QQ → tencent://message/?uin=2379727289" href=tencent://message/?uin=2379727289 rel=noopener target=_blank><i class="fab fa-qq fa-fw"></i>QQ</a> </span><span class=links-of-author-item> <a title="WeChat → weixin://dl/chat?HUGO--2025" href=weixin://dl/chat?HUGO--2025 rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>WeChat</a> </span><span class=links-of-author-item> <a title="Zhihu → https://www.zhihu.com/people/tong-ming-yun-bu-ji" href=https://www.zhihu.com/people/tong-ming-yun-bu-ji rel=noopener target=_blank><i class="fab fa-zhihu fa-fw"></i>Zhihu</a> </span><span class=links-of-author-item> <a title="Bilibili → https://space.bilibili.com/415423619?spm_id_from=333.788.0.0" href=https://space.bilibili.com/415423619?spm_id_from=333.788.0.0 rel=noopener target=_blank><i class="fas fa-tv-retro fa-fw"></i>Bilibili</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 好用的网站</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://csdiy.wiki/ rel=noopener target=_blank title=https://csdiy.wiki>csdiy</a><li class=links-of-blogroll-item><a href=https://opencs.app/ rel=noopener target=_blank title=https://opencs.app>opencs</a><li class=links-of-blogroll-item><a href=https://csrankings.org/ rel=noopener target=_blank title=https://csrankings.org>csrankings</a></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Hugo</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>170k</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>2:34</span></div><div class=footer-custom><div class=acknowledgment>Background photo by Shirley</div></div><div class=powered-by>由 <a class=theme-link href=https://hexo.io/ rel=noopener target=_blank>Hexo</a> & <a class=theme-link href=https://theme-next.org/ rel=noopener target=_blank>NexT.Gemini</a> 强力驱动</div><br><script async src=http://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
});</script><div class=busuanzi-count><script async src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script src=/lib/anime.min.js></script><script src=/lib/velocity/velocity.min.js></script><script src=/lib/velocity/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script src=/js/local-search.js></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><link href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css rel=stylesheet><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'Ov23li9o2o2qxTETe12J',
      clientSecret: '2a663bc459fc3e3df8c5a88e4753a7d0148034f3',
      repo        : 'blog-comments',
      owner       : 'Hugo0713',
      admin       : ['Hugo0713'],
      id          : '4a1708beddf9bb202dfe30a4642a869d',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script>